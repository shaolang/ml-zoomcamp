{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982542b0",
   "metadata": {},
   "source": [
    "# Homework 8\n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this homework, we'll build a model for predicting if we have an image of a dog or a cat. For this,\n",
    "we will use the \"Dogs & Cats\" dataset that can be downloaded from [Kaggle](https://www.kaggle.com/c/dogs-vs-cats/data). \n",
    "\n",
    "You need to download the `train.zip` file.\n",
    "\n",
    "If you have troubles downloading from Kaggle, use [this link](https://github.com/alexeygrigorev/large-datasets/releases/download/dogs-cats/train.zip) instead:\n",
    "\n",
    "```bash\n",
    "wget https://github.com/alexeygrigorev/large-datasets/releases/download/dogs-cats/train.zip\n",
    "```\n",
    "\n",
    "In the lectures we saw how to use a pre-trained neural network. In the homework, we'll train a much smaller model from scratch. \n",
    "\n",
    "**Note:** You don't need a computer with a GPU for this homework. A laptop or any personal computer should be sufficient. \n",
    "\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "The dataset contains 12,500 images of cats and 12,500 images of dogs. \n",
    "\n",
    "Now we need to split this data into train and validation\n",
    "\n",
    "* Create a `train` and `validation` folders\n",
    "* In each folder, create `cats` and `dogs` folders\n",
    "* Move the first 10,000 images to the train folder (from 0 to 9999) for boths cats and dogs - and put them in respective folders\n",
    "* Move the remaining 2,500 images to the validation folder (from 10000 to 12499)\n",
    "\n",
    "You can do this manually or with Python (check `os` and `shutil` packages).\n",
    "\n",
    "\n",
    "### Model\n",
    "\n",
    "For this homework we will use Convolutional Neural Network (CNN). Like in the lectures, we'll use Keras.\n",
    "\n",
    "You need to develop the model with following structure:\n",
    "\n",
    "* The shape for input should be `(150, 150, 3)`\n",
    "* Next, create a covolutional layer ([`Conv2D`](https://keras.io/api/layers/convolution_layers/convolution2d/)):\n",
    "    * Use 32 filters\n",
    "    * Kernel size should be `(3, 3)` (that's the size of the filter)\n",
    "    * Use `'relu'` as activation \n",
    "* Reduce the size of the feature map with max pooling ([`MaxPooling2D`](https://keras.io/api/layers/pooling_layers/max_pooling2d/))\n",
    "    * Set the pooling size to `(2, 2)`\n",
    "* Turn the multi-dimensional result into vectors using a [`Flatten`](https://keras.io/api/layers/reshaping_layers/flatten/) layer\n",
    "* Next, add a `Dense` layer with 64 neurons and `'relu'` activation\n",
    "* Finally, create the `Dense` layer with 1 neuron - this will be the output\n",
    "    * The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "\n",
    "As optimizer use [`SGD`](https://keras.io/api/optimizers/sgd/) with the following parameters:\n",
    "\n",
    "* `SGD(lr=0.002, momentum=0.8)`\n",
    "\n",
    "\n",
    "For clarification about kernel size and max pooling, check [Week #11 Office Hours](https://www.youtube.com/watch?v=1WRgdBTUaAc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee627ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=0.002, momentum=0.8), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4510c0",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Since we have a binary classification problem, what is the best loss function for us?\n",
    "\n",
    "Note: since we specify an activation for the output layer, we don't need to set `from_logits=True`\n",
    "\n",
    "**Answer:** [binary crossentropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cd194",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "What's the total number of parameters of the model? You can use the `summary` method for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54454296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 175232)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                11214912  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,215,873\n",
      "Trainable params: 11,215,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef2d90",
   "metadata": {},
   "source": [
    "## Generators and Training\n",
    "\n",
    "For the next two questions, use the following data generator for both train and validation:\n",
    "\n",
    "```python\n",
    "ImageDataGenerator(rescale=1./255)\n",
    "```\n",
    "\n",
    "* We don't need to do any additional pre-processing for the images.\n",
    "* When reading the data from train/val directories, check the `class_mode` parameter. Which value should it be for a binary classification problem?\n",
    "* Use `batch_size=20`\n",
    "\n",
    "For training use `.fit()` with the following params:\n",
    "\n",
    "```python\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50\n",
    ")\n",
    "```\n",
    "\n",
    "## Question 3\n",
    "\n",
    "What is the median of training accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6aee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a02fb392",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "What is the standard deviation of training loss for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a5f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55111b1e",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "For the next two questions, we'll generate more data using data augmentations. \n",
    "\n",
    "Add the following augmentations to your training data generator:\n",
    "\n",
    "* `rotation_range=40,`\n",
    "* `width_shift_range=0.2,`\n",
    "* `height_shift_range=0.2,`\n",
    "* `shear_range=0.2,`\n",
    "* `zoom_range=0.2,`\n",
    "* `horizontal_flip=True,`\n",
    "* `fill_mode='nearest'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527105e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f9c156f",
   "metadata": {},
   "source": [
    "## Question 5 \n",
    "\n",
    "Let's train our model for 10 more epochs using the same code as previously.\n",
    "Make sure you don't re-create the model - we want to continue training the model\n",
    "we already started training.\n",
    "\n",
    "What is the mean of validation loss for the model trained with augmentations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a397ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddb554d8",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "What's the average of validation accuracy for the last 5 epochs (from 6 to 10)\n",
    "for the model trained with augmentations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759c11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
